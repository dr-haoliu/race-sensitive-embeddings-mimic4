{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c81577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'data/cui_data_v3_with_trial_baseline_no_repetitaion_42776.pkl'\n",
    "\n",
    "# with open(filename, 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(len(b))\n",
    "# print(len(b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0727cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = b[0]\n",
    "# print(lines[:2])\n",
    "\n",
    "# print(len(lines))\n",
    "\n",
    "# with open('data/baseline_bert_train_raw_v1.txt', 'w') as f:\n",
    "#     for line in lines:\n",
    "#         line = line.replace('[CLS] ', '').replace(' [SEP] ', ' ')\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7356f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  9 12:44:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 59%   88C    P2   160W / 250W |   6580MiB / 11019MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 32%   29C    P8    17W / 250W |   2638MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 32%   34C    P8    24W / 250W |   2608MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0 Off |                  N/A |\n",
      "| 31%   27C    P8     6W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      7765      C   ...3/envs/hao_ner/bin/python     6577MiB |\n",
      "|    1   N/A  N/A       435      C   ...3/envs/hao_ner/bin/python     2635MiB |\n",
      "|    2   N/A  N/A      1309      C   ...3/envs/hao_ner/bin/python     2605MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f9e678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl2659/miniconda3/envs/hao_ner/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d52cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# t_model = \"bert-base-uncased\"\n",
    "model_type = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd3a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl2659/miniconda3/envs/hao_ner/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# input_data_file = '../data/raw_data_v3_without_trial_repetitaion_60198.txt'\n",
    "input_data_file = 'data/raw_data_v3_with_trial_repetitaion_155287.txt'\n",
    "\n",
    "from transformers import LineByLineTextDataset\n",
    "dataset = LineByLineTextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=input_data_file,\n",
    "        block_size=256)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a3569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl2659/miniconda3/envs/hao_ner/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 133110\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22186' max='22186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22186/22186 1:28:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.891100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.872300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.839600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_sensitive/checkpoint-5000\n",
      "Configuration saved in output_sensitive/checkpoint-5000/config.json\n",
      "Model weights saved in output_sensitive/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to output_sensitive/checkpoint-10000\n",
      "Configuration saved in output_sensitive/checkpoint-10000/config.json\n",
      "Model weights saved in output_sensitive/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to output_sensitive/checkpoint-15000\n",
      "Configuration saved in output_sensitive/checkpoint-15000/config.json\n",
      "Model weights saved in output_sensitive/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [output_sensitive/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to output_sensitive/checkpoint-20000\n",
      "Configuration saved in output_sensitive/checkpoint-20000/config.json\n",
      "Model weights saved in output_sensitive/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [output_sensitive/checkpoint-10000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22186, training_loss=0.9605989764812005, metrics={'train_runtime': 5300.0074, 'train_samples_per_second': 50.23, 'train_steps_per_second': 4.186, 'total_flos': 3.0445666663707e+16, 'train_loss': 0.9605989764812005, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_sensitive\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=12,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9013169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_sensitive/\n",
      "Configuration saved in output_sensitive/config.json\n",
      "Model weights saved in output_sensitive/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"output_sensitive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve pretrained model\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"output3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_model = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(t_model, use_fast=True)\n",
    "# import pickle\n",
    "# filename = 'data/cui_voc_pubmed_abtracts_v3.pickle'\n",
    "\n",
    "# with open(filename, 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(len(b))\n",
    "# blist = list(b)\n",
    "# blist = [x.lower() for x in blist]\n",
    "# blist.sort(key=str.lower)\n",
    "# blist[:10]\n",
    "# # Let's increase the vocabulary of Bert model and tokenizer\n",
    "# new_tokens = blist\n",
    "# num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "# print('We have added', num_added_toks, 'tokens')\n",
    "# # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45f6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets git+https://github.com/huggingface/transformers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c45fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9783cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'data/cui_voc_pubmed_abtracts_v3.pickle'\n",
    "\n",
    "# with open(filename, 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n",
    "# print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b835e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blist = list(b)\n",
    "# blist = [x.lower() for x in blist]\n",
    "# blist.sort(key=str.lower)\n",
    "# blist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd95ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's increase the vocabulary of Bert model and tokenizer\n",
    "# new_tokens = blist\n",
    "# num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "# print('We have added', num_added_toks, 'tokens')\n",
    "# # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'c0043541' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ffeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Here is the sentence I want embeddings for c0043541 .\"\n",
    "\n",
    "# marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# # Tokenize our sentence with the BERT tokenizer.\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# # Print out the tokens.\n",
    "# print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LineByLineTextDataset\n",
    "# dataset = LineByLineTextDataset(\n",
    "#         tokenizer=tokenizer,\n",
    "#         file_path=\"data/exist_cui_def_bert_train.txt\",\n",
    "#         block_size=256)\n",
    "\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff5b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"output3\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=10,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     save_steps=100,\n",
    "#     save_total_limit=2\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=dataset\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f21007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"output3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"output2\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74579342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file output_sensitive/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output_sensitive/pytorch_model.bin\n",
      "Some weights of the model checkpoint at output_sensitive/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at output_sensitive/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Retrieve SciBERT.\n",
    "model = BertModel.from_pretrained(\"output_sensitive/\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db9f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_indeces(tokenizer, text, word):\n",
    "    '''\n",
    "    Determines the index or indeces of the tokens corresponding to `word`\n",
    "    within `text`. `word` can consist of multiple words, e.g., \"cell biology\".\n",
    "    \n",
    "    Determining the indeces is tricky because words can be broken into multiple\n",
    "    tokens. I've solved this with a rather roundabout approach--I replace `word`\n",
    "    with the correct number of `[MASK]` tokens, and then find these in the \n",
    "    tokenized result. \n",
    "    '''\n",
    "    # Tokenize the 'word'--it may be broken into multiple tokens or subwords.\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    # Create a sequence of `[MASK]` tokens to put in place of `word`.\n",
    "    masks_str = ' '.join(['[MASK]']*len(word_tokens))\n",
    "\n",
    "    # Replace the word with mask tokens.\n",
    "    text_masked = text.replace(word, masks_str)\n",
    "\n",
    "    # `encode` performs multiple functions:\n",
    "    #   1. Tokenizes the text\n",
    "    #   2. Maps the tokens to their IDs\n",
    "    #   3. Adds the special [CLS] and [SEP] tokens.\n",
    "    input_ids = tokenizer.encode(text_masked)\n",
    "\n",
    "    # Use numpy's `where` function to find all indeces of the [MASK] token.\n",
    "    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n",
    "\n",
    "    return mask_token_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca338efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_embedding(b_model, b_tokenizer, text, word=''):\n",
    "    '''\n",
    "    Uses the provided model and tokenizer to produce an embedding for the\n",
    "    provided `text`, and a \"contextualized\" embedding for `word`, if provided.\n",
    "    '''\n",
    "\n",
    "    # If a word is provided, figure out which tokens correspond to it.\n",
    "    if not word == '':\n",
    "        word_indeces = get_word_indeces(b_tokenizer, text, word)\n",
    "\n",
    "    # Encode the text, adding the (required!) special tokens, and converting to\n",
    "    # PyTorch tensors.\n",
    "    encoded_dict = b_tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    b_model.eval()\n",
    "\n",
    "    # Run the text through the model and get the hidden states.\n",
    "    bert_outputs = b_model(input_ids)\n",
    "    \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = b_model(input_ids)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x <sentence length> x 768]\n",
    "\n",
    "    # Select the embeddings from the second to last layer.\n",
    "    # `token_vecs` is a tensor with shape [<sent length> x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    \n",
    "\n",
    "    # Calculate the average of all token vectors.\n",
    "#     sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #     sentence_embedding = torch.sum(token_vecs, dim=0)\n",
    "    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "    # cast layers to a tuple and concatenate over the last dimension\n",
    "    cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
    "#     print(cat_hidden_states.size())\n",
    "\n",
    "    # take the mean of the concatenated vector over the token dimension\n",
    "    cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "#     print(cat_sentence_embedding)\n",
    "#     print(cat_sentence_embedding.size())\n",
    "    sentence_embedding = cat_sentence_embedding\n",
    "\n",
    "    # Convert to numpy array.\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    # If `word` was provided, compute an embedding for those tokens.\n",
    "    if not word == '':\n",
    "        # Take the average of the embeddings for the tokens in `word`.\n",
    "        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)\n",
    "\n",
    "        # Convert to numpy array.\n",
    "        word_embedding = word_embedding.detach().numpy()\n",
    "    \n",
    "        return (sentence_embedding, word_embedding)\n",
    "    else:\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472c9368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # text = \"hydrogels are hydrophilic polymer networks which may absorb from 10â€“20% (an arbitrary lower limit) up to thousands of times their dry weight in water.\"\n",
    "# # word = 'hydrogels'\n",
    "\n",
    "# # text = 'C0007222 (CVDs) are the leading cause of death globally, taking an estimated 17.9 million lives each year. '\n",
    "# # word = 'C0007222'\n",
    "\n",
    "# text = 'C0007222'\n",
    "# word = 'C0007222'\n",
    "\n",
    "# tokenized_text = tokenizer.tokenize(text)\n",
    "# print(tokenized_text)\n",
    "\n",
    "# # Get the embedding for the sentence, as well as an embedding for 'hydrogels'.\n",
    "# (sen_emb, word_emb) = get_embedding(model, tokenizer, text, word)\n",
    "\n",
    "# print('Embedding sizes:')\n",
    "# print(sen_emb.shape)\n",
    "# print(word_emb.shape)\n",
    "# # Embedding sizes:\n",
    "# # (768,)\n",
    "# # (768,)\n",
    "# # Hereâ€™s the code for calculating cosine similarity. Weâ€™ll test it by\n",
    "# #  comparing the word embedding with the sentence embeddingâ€“not a very interesting comparison, but a good sanity check.\n",
    "\n",
    "# from scipy.spatial.distance import cosine\n",
    "\n",
    "# # Calculate the cosine similarity of the two embeddings.\n",
    "# sim = 1 - cosine(sen_emb, word_emb)\n",
    "\n",
    "# print('Cosine similarity: {:.2}'.format(sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C0085207\n",
    "# Gestational Diabetes\n",
    "# C0011849\n",
    "# Diabetes Mellitus\n",
    "# C0003811\n",
    "# Cardiac Arrhythmia\n",
    "# C0007222\n",
    "# Cardiovascular Diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e523fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cholera', 'due', 'to', 'vibrio', 'cholerae']\n",
      "['cholera', 'due', 'to', 'vibrio', 'cholerae', 'el', 'tor']\n",
      "[-0.0585447  -0.38258275 -0.15768597 -0.06286092 -0.39517486]\n",
      "[ 0.24277705  0.05865506 -0.03173161  0.02194069 -0.41628045]\n",
      "word Cosine similarity: 0.8596\n",
      "sent Cosine similarity: 0.9817\n",
      "[-0.09673499 -0.01326337 -0.15043162  0.11172631 -0.2900265 ]\n",
      "[-0.10404948  0.01963876 -0.10541455  0.03605231 -0.34910363]\n"
     ]
    }
   ],
   "source": [
    "# text1 = 'C0003811 Cardiac Arrhythmia'\n",
    "# word1 = 'C0003811'\n",
    "# text2 = 'C0007222 Cardiovascular Diseases'\n",
    "# word2 = 'C0007222'\n",
    "# text1 = 'Gestational Diabetes'\n",
    "# word1 = 'Diabetes'\n",
    "# text2 = 'Diabetes Mellitus'\n",
    "# word2 = 'Mellitus'\n",
    "text1 = 'Cholera due to vibrio cholerae'\n",
    "word1 = 'Cholera'\n",
    "# text1 = 'Cardiac Arrhythmia'\n",
    "# word1 = 'Cardiac'\n",
    "# text2 = 'Cardiovascular Diseases'  # Diabetes Mellitus\n",
    "# word2 = 'Cardiovascular'\n",
    "text2 = 'Cholera due to vibrio cholerae el tor'\n",
    "word2 = 'vibrio'\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text1)\n",
    "print(tokenized_text)\n",
    "tokenized_text = tokenizer.tokenize(text2)\n",
    "print(tokenized_text)\n",
    "\n",
    "sen_emb1, word_emb1 = get_embedding(model, tokenizer, text1, word1)\n",
    "sen_emb2, word_emb2 = get_embedding(model, tokenizer, text2, word2)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(word_emb1[:5])\n",
    "print(word_emb2[:5])\n",
    "\n",
    "# Calculate the cosine similarity of the two embeddings.\n",
    "sim = 1.0 - cosine(word_emb1, word_emb2)\n",
    "# print(sim)\n",
    "print('word Cosine similarity: {:.4}'.format(sim))\n",
    "\n",
    "sim = 1.0 - cosine(sen_emb1, sen_emb2)\n",
    "# print(sim)\n",
    "print('sent Cosine similarity: {:.4}'.format(sim))\n",
    "\n",
    "print(sen_emb1[:5])\n",
    "print(sen_emb2[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
